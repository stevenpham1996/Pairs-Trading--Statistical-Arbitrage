{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from pytickersymbols import PyTickerSymbols\n",
    "from pandas_datareader.stooq import StooqDailyReader\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research, the stock symbols of the S&P500 composite will be selected by their sectors and current bidding prices (smaller than 500 USD).  \n",
    "  \n",
    "**NOTE**: the selection of the S&P500 stock universe is not accounted for Survivorship Bias. *This is not expected to affect the results of our arbitrage strategy as the strategy does not rely on the performances of the individual stocks*.\n",
    "\n",
    "Data: 13 years of historical Daily Opens and Closes from Stooq.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stocks_by_sector_and_price(sectors_list, max_price):\n",
    "    # create a PyTickerSymbols object\n",
    "    stock_data = PyTickerSymbols()\n",
    "\n",
    "    # get all the stocks of the S&P 500 index\n",
    "    sp500_stocks = stock_data.get_stocks_by_index(\"S&P 500\")\n",
    "\n",
    "    # create a dictionary to store the industries\n",
    "    stocks_from_sp500 = {}\n",
    "\n",
    "    # loop through the stocks and get the ticker and industry for each one\n",
    "    for stock in sp500_stocks:\n",
    "        # get the ticker symbol from the symbol attribute\n",
    "        ticker = stock[\"symbol\"]\n",
    "        # get the industry from the industries attribute\n",
    "        industry = stock[\"industries\"]\n",
    "        # convert the industry to a string\n",
    "        industry = str(industry)\n",
    "        # use a regex to match the unique industry\n",
    "        pattern = r\"(\" + \"|\".join(sectors_list) + \")\"\n",
    "        match = re.search(pattern, industry)\n",
    "        # check if the match object is not None\n",
    "        if match is not None:\n",
    "            matched_industry = match.group(0)\n",
    "            # if the industry is not in the dictionary, add it with an empty list\n",
    "            if matched_industry not in stocks_from_sp500:\n",
    "                stocks_from_sp500[matched_industry] = []\n",
    "            # append the ticker to the list of the corresponding industry\n",
    "            stocks_from_sp500[matched_industry].append(ticker)\n",
    "\n",
    "    for _, stocks in stocks_from_sp500.items():\n",
    "        for ticker in stocks:\n",
    "            try:\n",
    "                data = yf.Ticker(ticker).history(raise_errors=True)['Close']\n",
    "                if data is not None or len(data) > 0:\n",
    "                    price = data.iloc[-1]\n",
    "                    if price >= max_price:\n",
    "                        stocks_from_sp500[matched_industry].remove(ticker)\n",
    "            except Exception as _:\n",
    "                continue\n",
    "\n",
    "    # loop through the items of the dictionary and print them\n",
    "    total_stocks = 0\n",
    "    for industry, stocks in stocks_from_sp500.items():\n",
    "        # print the industry and a newline character\n",
    "        print(industry + \":\\n\")\n",
    "        # print the list of stocks and a newline character\n",
    "        print(stocks, \"\\n\")\n",
    "        print('numer of stocks', len(stocks), \"\\n\")\n",
    "        total_stocks += len(stocks)\n",
    "        print(\"--------------------------------------------------\\n\")\n",
    "    print('total stocks: ', total_stocks, \"\\n\")\n",
    "\n",
    "    return stocks_from_sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materials:\n",
      "\n",
      "['LIN', 'DOW', 'APD', 'ALB', 'AVY', 'BLL', 'CE', 'CF', 'EMN', 'ECL', 'FMC', 'FCX', 'LYB', 'MLM', 'MOS', 'NEM', 'NUE', 'PKG', 'PPG', 'SEE', 'SHW', 'VMC', 'WRK', 'CTVA'] \n",
      "\n",
      "numer of stocks 24 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Consumer:\n",
      "\n",
      "['CCL', 'HD', 'MCD', 'PG', 'WMT', 'WBA', 'DIS', 'AMZN', 'CHTR', 'CMCSA', 'DISH', 'DLTR', 'EXPE', 'MAR', 'ROST', 'SBUX', 'TSLA', 'TSCO', 'ULTA', 'CL', 'F', 'GM', 'LOW', 'TGT', 'AAP', 'ABC', 'BBY', 'BWA', 'CAH', 'KMX', 'CHD', 'CLX', 'CPRT', 'CMI', 'DRI', 'DG', 'DHI', 'FBHS', 'FOXA', 'GPC', 'HAS', 'HLT', 'IPG', 'KMB', 'KR', 'LEN', 'LKQ', 'MAS', 'MCK', 'MGM', 'MHK', 'NWL', 'NWSA', 'NCLH', 'OMC', 'PHM', 'RL', 'RCL', 'SYY', 'TTWO', 'TPR', 'TJX', 'VFC', 'WHR', 'WYNN', 'YUM', 'META'] \n",
      "\n",
      "numer of stocks 67 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Industrials:\n",
      "\n",
      "['MMM', 'BA', 'CAT', 'AAL', 'CSX', 'FAST', 'FISV', 'PCAR', 'PAYX', 'EMR', 'FDX', 'HON', 'RTN', 'UNP', 'UPS', 'AMD', 'ALK', 'AME', 'AOS', 'BR', 'CHRW', 'CTAS', 'DE', 'DAL', 'DOV', 'ETN', 'EFX', 'EXPD', 'FLT', 'FTV', 'GRMN', 'GPN', 'GWW', 'HII', 'ITW', 'JBHT', 'JEC', 'JCI', 'KEYS', 'MTD', 'MCO', 'MSCI', 'PH', 'PNR', 'PEG', 'PWR', 'RSG', 'RHI', 'ROK', 'ROL', 'ROP', 'SPGI', 'LUV', 'SWK', 'TEL', 'UAL', 'URI', 'WAB', 'WM', 'XYL', 'TDG'] \n",
      "\n",
      "numer of stocks 61 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Financial:\n",
      "\n",
      "['AXP', 'GS', 'JPM', 'TRV', 'ALL', 'AIG', 'BAC', 'BK', 'BRK-B', 'BLK', 'COF', 'C', 'MET', 'MS', 'USB', 'WFC', 'AFL', 'AMP', 'AON', 'AJG', 'AIZ', 'CB', 'CINF', 'CFG', 'CME', 'CMA', 'DFS', 'RE', 'FIS', 'FITB', 'FRC', 'BEN', 'HIG', 'HBAN', 'ICE', 'IVZ', 'LNC', 'L', 'MTB', 'MMC', 'NDAQ', 'NTRS', 'PNC', 'PGR', 'PRU', 'RJF', 'RF', 'SCHW', 'STT', 'SIVB', 'SYF', 'TROW', 'WLTW', 'ZION', 'KEY', 'TFC', 'FDS'] \n",
      "\n",
      "numer of stocks 57 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Technology:\n",
      "\n",
      "['AAPL', 'CSCO', 'IBM', 'INTC', 'MSFT', 'V', 'ATVI', 'ADBE', 'AKAM', 'GOOGL', 'ADI', 'AMAT', 'ADSK', 'ADP', 'CTSH', 'EBAY', 'EA', 'INTU', 'LRCX', 'MCHP', 'MU', 'NTAP', 'NFLX', 'NVDA', 'NXPI', 'PYPL', 'QCOM', 'STX', 'SWKS', 'TXN', 'VRSK', 'WDC', 'ACN', 'MA', 'ORCL', 'ALLE', 'APH', 'ANSS', 'APTV', 'ANET', 'AVGO', 'CDNS', 'GLW', 'DXC', 'FFIV', 'FTNT', 'IT', 'HPE', 'HPQ', 'JKHY', 'JNPR', 'KLAC', 'MSI', 'QRVO', 'CRM', 'SNA', 'SNPS', 'VRSN', 'CSGP'] \n",
      "\n",
      "numer of stocks 59 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Energy:\n",
      "\n",
      "['CVX', 'XOM', 'COP', 'KMI', 'OXY', 'SLB', 'APA', 'DVN', 'FANG', 'EOG', 'HAL', 'HES', 'IFF', 'IP', 'MRO', 'MPC', 'OKE', 'PSX', 'PXD', 'VLO', 'WMB', 'ENPH'] \n",
      "\n",
      "numer of stocks 22 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Food:\n",
      "\n",
      "['KO', 'KHC', 'MDLZ', 'MNST', 'MO', 'ADM', 'BF-B', 'CPB', 'CAG', 'STZ', 'GIS', 'HSY', 'HRL', 'SJM', 'K', 'LW', 'MKC', 'TSN', 'DPZ', 'TAP'] \n",
      "\n",
      "numer of stocks 20 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Pharmaceuticals:\n",
      "\n",
      "['JNJ', 'MRK', 'PFE', 'AMGN', 'BIIB', 'GILD', 'INCY', 'REGN', 'VRTX', 'ABBV', 'BMY', 'LLY', 'IQV', 'ZTS', 'VTRS', 'WST', 'CRL'] \n",
      "\n",
      "numer of stocks 17 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Healthcare:\n",
      "\n",
      "['UNH', 'XRAY', 'HSIC', 'ILMN', 'ISRG', 'ABT', 'CVS', 'DHR', 'MDT', 'PM', 'ABMD', 'A', 'ALGN', 'BAX', 'BDX', 'BSX', 'CNC', 'CI', 'COO', 'DVA', 'EW', 'HCA', 'HOLX', 'HUM', 'IDXX', 'LH', 'PKI', 'DGX', 'RMD', 'SYK', 'TFX', 'TMO', 'UHS', 'WAT', 'ZBH', 'MOH'] \n",
      "\n",
      "numer of stocks 36 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Real Estate:\n",
      "\n",
      "['SPG', 'ARE', 'AMT', 'AVB', 'BXP', 'CBRE', 'ED', 'CCI', 'DLR', 'EQIX', 'EQR', 'ESS', 'EXR', 'FRT', 'PEAK', 'HST', 'IRM', 'KIM', 'MAA', 'PLD', 'PSA', 'O', 'SBAC', 'UDR', 'VTR', 'VNO', 'WELL', 'WY'] \n",
      "\n",
      "numer of stocks 28 \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "total stocks:  391 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sectors_list = ['Industrials', 'Consumer', 'Financial', 'Energy', 'Real Estate', 'Food', 'Pharmaceuticals', 'Healthcare', 'Materials']\n",
    "tickers_by_sectors = get_stocks_by_sector_and_price(sectors_list, max_price=500.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:  \n",
    "Due to the [After Hour Trading Gap](https://www.investopedia.com/terms/g/gap.asp) phenomena that is inherent to the stock market, the common way of calculating daily returns by using today Close and yesterday Close should be avoided.  \n",
    "In order to best appropriately model daily returns that is the most realistic to live trading, the daily log returns should be calculated as the following: *today Close - today Open*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #NOTE: remove path later\n",
    "# prices = pd.read_csv(r\"C:\\Users\\Steven\\OneDrive\\FreeLance Works\\Pairs-Trading-Statistical-Arbitrage\\US_NYSE_Daily_2010.csv\", parse_dates=[0], index_col=[0], engine='c')\n",
    "# # get the selected tickers from the S&P 500\n",
    "# tickers = []\n",
    "# for _, stocks in tickers_by_sectors.items():\n",
    "#     tickers.extend(stocks)\n",
    "# # download historical data from stooq.com\n",
    "# end = datetime.now().date()\n",
    "# start = datetime(2010, 1, 1).date()\n",
    "# data = StooqDailyReader(tickers, start, end, chunksize=100).read()\n",
    "# data.drop(['High', 'Low', 'Volume'], axis=1, inplace=True)\n",
    "# prices = data[::-1].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Attributes</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Close</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbols</th>\n",
       "      <th>LIN</th>\n",
       "      <th>DOW</th>\n",
       "      <th>APD</th>\n",
       "      <th>ALB</th>\n",
       "      <th>AVY</th>\n",
       "      <th>CE</th>\n",
       "      <th>CF</th>\n",
       "      <th>EMN</th>\n",
       "      <th>ECL</th>\n",
       "      <th>FMC</th>\n",
       "      <th>...</th>\n",
       "      <th>MAA</th>\n",
       "      <th>PLD</th>\n",
       "      <th>PSA</th>\n",
       "      <th>O</th>\n",
       "      <th>SBAC</th>\n",
       "      <th>UDR</th>\n",
       "      <th>VTR</th>\n",
       "      <th>VNO</th>\n",
       "      <th>WELL</th>\n",
       "      <th>WY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.5609</td>\n",
       "      <td>32.0135</td>\n",
       "      <td>28.0723</td>\n",
       "      <td>27.3038</td>\n",
       "      <td>13.9217</td>\n",
       "      <td>22.7835</td>\n",
       "      <td>39.5997</td>\n",
       "      <td>21.1501</td>\n",
       "      <td>...</td>\n",
       "      <td>32.4312</td>\n",
       "      <td>22.0269</td>\n",
       "      <td>52.5185</td>\n",
       "      <td>15.5413</td>\n",
       "      <td>33.4733</td>\n",
       "      <td>11.1372</td>\n",
       "      <td>22.7042</td>\n",
       "      <td>31.3970</td>\n",
       "      <td>25.7504</td>\n",
       "      <td>10.4915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.0518</td>\n",
       "      <td>31.9568</td>\n",
       "      <td>28.2560</td>\n",
       "      <td>27.9106</td>\n",
       "      <td>14.1450</td>\n",
       "      <td>22.4922</td>\n",
       "      <td>38.9950</td>\n",
       "      <td>21.0768</td>\n",
       "      <td>...</td>\n",
       "      <td>31.8861</td>\n",
       "      <td>21.8828</td>\n",
       "      <td>51.3045</td>\n",
       "      <td>15.6182</td>\n",
       "      <td>34.5350</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>22.2132</td>\n",
       "      <td>31.1412</td>\n",
       "      <td>25.6165</td>\n",
       "      <td>10.5870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.5478</td>\n",
       "      <td>32.0135</td>\n",
       "      <td>28.1909</td>\n",
       "      <td>28.2166</td>\n",
       "      <td>14.7369</td>\n",
       "      <td>22.3411</td>\n",
       "      <td>38.8811</td>\n",
       "      <td>21.2808</td>\n",
       "      <td>...</td>\n",
       "      <td>31.9520</td>\n",
       "      <td>21.8211</td>\n",
       "      <td>51.8577</td>\n",
       "      <td>15.7409</td>\n",
       "      <td>35.0336</td>\n",
       "      <td>10.9063</td>\n",
       "      <td>22.3710</td>\n",
       "      <td>30.9236</td>\n",
       "      <td>25.8881</td>\n",
       "      <td>10.5950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.2015</td>\n",
       "      <td>31.7856</td>\n",
       "      <td>28.4575</td>\n",
       "      <td>28.3646</td>\n",
       "      <td>14.5896</td>\n",
       "      <td>22.3948</td>\n",
       "      <td>39.2998</td>\n",
       "      <td>21.0174</td>\n",
       "      <td>...</td>\n",
       "      <td>31.8802</td>\n",
       "      <td>22.0906</td>\n",
       "      <td>51.3120</td>\n",
       "      <td>15.7321</td>\n",
       "      <td>34.7894</td>\n",
       "      <td>10.7856</td>\n",
       "      <td>22.0544</td>\n",
       "      <td>30.6657</td>\n",
       "      <td>25.7464</td>\n",
       "      <td>10.6461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.5882</td>\n",
       "      <td>32.2654</td>\n",
       "      <td>29.0009</td>\n",
       "      <td>28.0992</td>\n",
       "      <td>14.9019</td>\n",
       "      <td>22.4571</td>\n",
       "      <td>39.6323</td>\n",
       "      <td>20.8292</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0179</td>\n",
       "      <td>22.1552</td>\n",
       "      <td>51.1564</td>\n",
       "      <td>15.9659</td>\n",
       "      <td>35.5800</td>\n",
       "      <td>10.7856</td>\n",
       "      <td>22.2200</td>\n",
       "      <td>30.9868</td>\n",
       "      <td>25.6165</td>\n",
       "      <td>10.5763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 758 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Attributes Close                                                            \\\n",
       "Symbols      LIN DOW      APD      ALB      AVY       CE       CF      EMN   \n",
       "2010-01-04   NaN NaN  61.5609  32.0135  28.0723  27.3038  13.9217  22.7835   \n",
       "2010-01-05   NaN NaN  61.0518  31.9568  28.2560  27.9106  14.1450  22.4922   \n",
       "2010-01-06   NaN NaN  60.5478  32.0135  28.1909  28.2166  14.7369  22.3411   \n",
       "2010-01-07   NaN NaN  60.2015  31.7856  28.4575  28.3646  14.5896  22.3948   \n",
       "2010-01-08   NaN NaN  60.5882  32.2654  29.0009  28.0992  14.9019  22.4571   \n",
       "\n",
       "Attributes                    ...     Open                             \\\n",
       "Symbols         ECL      FMC  ...      MAA      PLD      PSA        O   \n",
       "2010-01-04  39.5997  21.1501  ...  32.4312  22.0269  52.5185  15.5413   \n",
       "2010-01-05  38.9950  21.0768  ...  31.8861  21.8828  51.3045  15.6182   \n",
       "2010-01-06  38.8811  21.2808  ...  31.9520  21.8211  51.8577  15.7409   \n",
       "2010-01-07  39.2998  21.0174  ...  31.8802  22.0906  51.3120  15.7321   \n",
       "2010-01-08  39.6323  20.8292  ...  32.0179  22.1552  51.1564  15.9659   \n",
       "\n",
       "Attributes                                                        \n",
       "Symbols        SBAC      UDR      VTR      VNO     WELL       WY  \n",
       "2010-01-04  33.4733  11.1372  22.7042  31.3970  25.7504  10.4915  \n",
       "2010-01-05  34.5350  10.8917  22.2132  31.1412  25.6165  10.5870  \n",
       "2010-01-06  35.0336  10.9063  22.3710  30.9236  25.8881  10.5950  \n",
       "2010-01-07  34.7894  10.7856  22.0544  30.6657  25.7464  10.6461  \n",
       "2010-01-08  35.5800  10.7856  22.2200  30.9868  25.6165  10.5763  \n",
       "\n",
       "[5 rows x 758 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "prices = pd.read_csv(r\"C:\\Users\\Steven\\OneDrive\\FreeLance Works\\Pairs-Trading-Statistical-Arbitrage\\copulas_pairs\\SP500_stock_data.csv\", parse_dates=[0], index_col=[0], header=[0,1], engine='c')\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy algorithm will be demonstrated in ordered steps in this section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pair Selection\n",
    "The first step is identifying potential pairs for trading.  \n",
    "\n",
    "Pairs in top *90th percentile* of the highest [Kendall’s rank correlation coefficient](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) (Kendall’s tau) between log-returns will be selected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rank_correlation(log_returns: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Kendall’s rank correlation coefficient statistic for pairwise comparison of columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        log_returns (pd.DataFrame): A DataFrame containing log returns of assets.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A Series containing the Kendall’s rank correlation coefficient statistic for pairwise comparison of columns.\n",
    "    \"\"\"\n",
    "    tau_ser = pd.Series(name=\"tau\")\n",
    "    for s1 in log_returns.columns:\n",
    "        for s2 in log_returns.columns:\n",
    "            if (s1 != s2) and (f\"{s2}-{s1}\" not in tau_ser.index):\n",
    "                try:\n",
    "                    tau_ser.loc[f\"{s1}-{s2}\"] = stats.kendalltau(\n",
    "                        log_returns[s1].values, log_returns[s2].values\n",
    "                    )[0]\n",
    "                except:\n",
    "                    continue\n",
    "    return tau_ser\n",
    "\n",
    "\n",
    "def parse_pair(pair):\n",
    "    \"\"\"\n",
    "    Parse a pair string into two separate strings.\n",
    "\n",
    "    Parameters:\n",
    "        pair (str): A string representing a pair of values separated by a hyphen.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two strings, the first string before the hyphen and the second string after the hyphen.\n",
    "    \"\"\"\n",
    "    s1 = pair[: pair.find(\"-\")]\n",
    "    s2 = pair[pair.find(\"-\") + 1 :]\n",
    "    return s1, s2\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame):\n",
    "    \"\"\"Clean and log-transform the price dataframe.\"\"\"  \n",
    "    # Impute missing or infinite values\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Interpolate missing values\n",
    "    data.interpolate(source='spline', order=5, limit_direction='both', inplace=True)\n",
    "    # drop nan values\n",
    "    data.dropna(inplace=True)\n",
    "    # Compute Log-Returns\n",
    "    try:\n",
    "        df = np.log(data)\n",
    "        log_returns = pd.DataFrame(columns=df.columns.get_level_values(1).unique(), index=df.index)\n",
    "        for col in df.columns.get_level_values(1).unique():\n",
    "            close_prices = df.Close[col]\n",
    "            open_prices = df.Open[col]\n",
    "            log_returns[col] =  close_prices - open_prices\n",
    "        return log_returns\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Log-transforming data FAILED!!!\\\n",
    "                \\n{type(e).__name__}: {str(e)}\\n\")\n",
    "        return     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prices.iloc[:252*3] # create train dataset of 3 years\n",
    "\n",
    "log_returns = preprocess_data(train_data)\n",
    "kendall_T = calc_rank_correlation(log_returns)\n",
    "top_kendall_T = kendall_T[kendall_T > kendall_T.quantile(0.9)]\n",
    "\n",
    "# Select pairs\n",
    "selected_assets = []\n",
    "selected_pairs = []\n",
    "for pair in reversed(top_kendall_T.index):\n",
    "    s1, s2 = parse_pair(pair)\n",
    "    if (s1 not in selected_assets) and (s2 not in selected_assets):\n",
    "        selected_assets.append(s1)\n",
    "        selected_assets.append(s2)\n",
    "        selected_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(selected_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit Marginals  \n",
    "Find the best fitted marginal distributions for individual currencies within each pair.\n",
    "The five following four parametric distributions will be used: Normal, Laplace, Student’s t, Logistic and Extreme.  \n",
    "The best fitted distributions will be chosen based on Bayesian Information Criterion (BIC), and then those that pass the Kolmogorov-Smirnov goodness-of-fit test will be carried on as our selected distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_marginals(\n",
    "    log_returns: pd.DataFrame, selected_assets: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fits marginal distributions to the given log returns data for selected assets.\n",
    "\n",
    "    Parameters:\n",
    "    - log_returns: A pandas DataFrame containing the log returns data.\n",
    "    - selected_assets: A list of strings representing the selected assets.\n",
    "\n",
    "    Returns:\n",
    "    - fitted_marginals: A dictionary mapping each stock to the best fitted marginal distribution.\n",
    "    \"\"\"\n",
    "    fitted_marginals = {}\n",
    "    for stock in selected_assets:\n",
    "        \n",
    "        data = log_returns[stock]\n",
    "        best_pval = 0.0\n",
    "\n",
    "        for dist in [\n",
    "            stats.norm,\n",
    "            stats.laplace,\n",
    "            stats.t,\n",
    "            stats.genhyperbolic,\n",
    "            stats.genextreme,\n",
    "        ]:\n",
    "            try:\n",
    "                params = dist.fit(data)\n",
    "                dist_fit = dist(*params)\n",
    "                ks_pval = stats.kstest(data, dist_fit.cdf, N=len(data))[1]\n",
    "                if ks_pval > best_pval:\n",
    "                    best_dist = dist_fit\n",
    "                    best_pval = ks_pval\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        fitted_marginals[stock] = best_dist\n",
    "\n",
    "    return fitted_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_marginals = fit_marginals(log_returns, selected_assets)\n",
    "# First 5 fitted marginals\n",
    "for stock, dist in list(fitted_marginals.items())[:5]:\n",
    "    print(f\"Stock: '{stock}', Marginal Distribution: {dist.dist.name.upper()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit Copulas  \n",
    "First, individual assets' log returns are converted to their Uniform samples (CDFs) via applying the Probability Integral Transform to their best fitted marginal distributions earlier.  \n",
    "Then fitting each pair's log return CDFs to seven Copula types: Gaussian, N13, N14, Clayton, Gumbel, Frank and Joe.\n",
    "Copulas with the either smallest Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) will be selected for the two-dimensional Kolmogorov-Smirnov test.  \n",
    "The two-dimensional Kolmogorov-Smirnov test compares the join-CDFs between 2 pairs of marginal distributions and the Null Hypothesis is that they are identical (the github repo of its Python implementation is [here](https://github.com/syrte/ndtest/blob/master/ndtest.py)).  \n",
    "The top **10** Copulas which pass the 2d-KS test with the highest P-values will beour selected Copulas for trading.  \n",
    "  \n",
    "  \n",
    "(OR The top **10** Copulas which pass the 2d-KS test with the highest P-values while also have the lowest AIC and BIC scores will be our selected Copulas for trading.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copulas # Thanks to courtesy of Alexander Pavlov at https://github.com/financialnoob\n",
    "import ndtest\n",
    "\n",
    "def fit_copulas(\n",
    "    log_returns: pd.DataFrame, fitted_marginals: dict, selected_pairs: list[tuple[str]]\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Fits copulas to log returns data.\n",
    "\n",
    "    Args:\n",
    "        log_returns (pd.DataFrame): A DataFrame containing log returns data.\n",
    "        fitted_marginals (dict): A dictionary of fitted marginal distributions.\n",
    "        selected_pairs (list): A list of selected pairs of assets.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, dict]: A tuple containing two objects:\n",
    "            - copulas_df: A DataFrame with fit statistics for each pair of assets.\n",
    "            - fitted_copulas: A dictionary of fitted copula models for each pair of assets.\n",
    "    \"\"\"\n",
    "    copulas_df = pd.DataFrame(index=selected_pairs, columns=[\"KS_pvalue\"])\n",
    "    fitted_copulas = {}\n",
    "\n",
    "    for pair in selected_pairs:\n",
    "        s1, s2 = parse_pair(pair)\n",
    "\n",
    "        dist_s1 = fitted_marginals[s1]\n",
    "        dist_s2 = fitted_marginals[s2]\n",
    "\n",
    "        # apply probability integral transform\n",
    "        u = dist_s1.cdf(log_returns[s1])\n",
    "        v = dist_s2.cdf(log_returns[s2])\n",
    "\n",
    "        best_aic = np.inf\n",
    "        best_bic = np.inf\n",
    "        best_copula = None\n",
    "\n",
    "        for copula in [\n",
    "            copulas.GaussianCopula(),\n",
    "            copulas.N13Copula(),\n",
    "            copulas.N14Copula(),\n",
    "            copulas.ClaytonCopula(),\n",
    "            copulas.GumbelCopula(),\n",
    "            copulas.FrankCopula(),\n",
    "            copulas.JoeCopula(),\n",
    "        ]:\n",
    "            try:\n",
    "                copula.fit(u, v)\n",
    "                L = copula.log_likelihood(u, v)\n",
    "                aic = 2 * copula.num_params - 2 * L\n",
    "                bic = copula.num_params * np.log(len(u)) - 2 * L\n",
    "                if aic < best_aic or bic < best_bic:\n",
    "                    best_aic = aic\n",
    "                    best_bic = bic\n",
    "                    best_copula = copula\n",
    "                    # calculate KS-pvalue\n",
    "                    smp = best_copula.sample(size=len(u))  # generate sample from fit copula\n",
    "                    s_u = smp[:, 0]\n",
    "                    s_v = smp[:, 1]\n",
    "                    ks_pval = ndtest.ks2d2s(u, v, s_u, s_v)\n",
    "                    if ks_pval > 0.2: # 20% threshold indicates significant as the authors recommended.\n",
    "                        copulas_df.loc[pair, \"KS_pvalue\"] = ks_pval\n",
    "                        fitted_copulas[pair] = {\"s1\": dist_s1, \"s2\": dist_s2, \"copula\": best_copula}\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return copulas_df, fitted_copulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Copulas satisfied the 2D-KS test.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KS_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [KS_pvalue]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copulas_df, fitted_copulas = fit_copulas(log_returns, fitted_marginals, selected_pairs)\n",
    "if copulas_df.empty:\n",
    "    print(\"No Copulas satisfied the 2D-KS test.\")\n",
    "# sort the copulas_df by the lowest aic, bic and the highest KS_pvalue\n",
    "# sorted_copulas_df = copulas_df.sort_values(\n",
    "#     by=[\"KS_pvalue\", \"bic\", \"aic\"], ascending=[False, True, True]\n",
    "# )\n",
    "# sort the copulas_df by the highest KS_pvalue\n",
    "sorted_copulas_df = copulas_df.sort_values(\n",
    "    by=[\"KS_pvalue\"], ascending=[False]\n",
    ")\n",
    "\n",
    "top_copulas_df = sorted_copulas_df.iloc[:10]\n",
    "\n",
    "top_copulas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the trading algorithm's processes, let's neatly combine them into a single function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formation(data: pd.DataFrame, use_num=False, num_pairs=10, top_pct: int = 0.9) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Generate a formation based on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A pandas DataFrame containing the input data.\n",
    "    - top_pct: An integer representing the top percentile of tau values\n",
    "    and the best fitted marginals and corresponding copulas.\n",
    "\n",
    "    Returns:\n",
    "    - result_dict: A dictionary of the selected pairs of assets\n",
    "    and the best fitted marginals and corresponding copulas.\n",
    "    \"\"\"\n",
    "    log_returns = preprocess_data(data)\n",
    "    hoeffdingD_ser = calc_rank_correlation(log_returns)\n",
    "    top_hoeffdingD_ser = hoeffdingD_ser[hoeffdingD_ser > hoeffdingD_ser.quantile(top_pct)]\n",
    "\n",
    "    selected_assets = []\n",
    "    selected_pairs = []\n",
    "    for pair in top_hoeffdingD_ser.index:\n",
    "        s1, s2 = parse_pair(pair)\n",
    "        if (s1 not in selected_assets) and (s2 not in selected_assets):\n",
    "            selected_assets.append(s1)\n",
    "            selected_assets.append(s2)\n",
    "            selected_pairs.append(pair)\n",
    "\n",
    "    fitted_marginals = fit_marginals(log_returns, selected_assets)\n",
    "    copulas_df, fitted_copulas = fit_copulas(log_returns, fitted_marginals, selected_pairs)\n",
    "    # if all failed 2d-KS-test's criteria, return empty dict\n",
    "    if copulas_df.empty:\n",
    "        return {}\n",
    "    # sort the copulas_df by the highest KS_pvalue\n",
    "    sorted_copulas_df = copulas_df.sort_values(\n",
    "        by=[\"KS_pvalue\"], ascending=[False]\n",
    "    )\n",
    "    if use_num:\n",
    "        top_copulas_df = sorted_copulas_df.iloc[:num_pairs]\n",
    "    else: # get the top percentile\n",
    "        p90 = int(len(sorted_copulas_df) * top_pct)\n",
    "        top_copulas_df = sorted_copulas_df.iloc[:p90]\n",
    "    # match the dictionaries in the dictionary fitted_copulas\n",
    "    # with the top 90 percentile in top_copulas_df\n",
    "    result_dict = {}\n",
    "    for pair in top_copulas_df.index:\n",
    "        if pair in fitted_copulas.keys():\n",
    "            result_dict[pair] = fitted_copulas[pair]\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest\n",
    "Trade decisions are made based on the conditional probabilities of each cryptocurrency pairs' log-returns. By defining Upper and Lower thresholds ($b_{up}$ & $b_{lo}$), we can determine the current return of one stock is too high, given the second stock's current return. This is demonstrated as the following:\n",
    "- **Opening** rules:  \n",
    "-- IF $P(U_1\\le u_1 | U_2 = u_2) \\le b_{lo}$ AND $P(U_2\\le u_2 | U_1 = u_1) \\ge b_{up}$,  \n",
    " then stock 1 is undervalued, and stock 2 is overvalued. Hence we long the spread. (1 in position)  \n",
    "-- IF $P(U_2\\le u_2 | U_1 = u_1) \\le b_{lo}$ AND $P(U_1\\le u_1 | U_2 = u_2) \\ge b_{up}$,    \n",
    " then stock 2 is undervalued, and stock 1 is overvalued. Hence we short the spread. (-1 in position)\n",
    "- **Exit** rule:  \n",
    "If BOTH/EITHER conditional probabilities cross the boundary of 0.5, then we exit the position, as we consider the position no longer valid. (0 in position). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology: \n",
    "Select the \n",
    "Lower Bound: $b_{lo}$ = **0.05**  \n",
    "Upper Bound: $b_{up}$ = **0.95**  \n",
    "Training period length: **3 years**.  \n",
    "Trading period length: **3 months**.  \n",
    "Leverage ratio: **1:20**.  \n",
    "Transaction & commission fees: **0.25% per position**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2010-01-04 00:00:00 to 2013-01-04 00:00:00\n",
      "Test period: 2013-01-04 00:00:00 to 2013-04-09 00:00:00\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2010-04-06 00:00:00 to 2013-04-08 00:00:00\n",
      "Test period: 2013-04-08 00:00:00 to 2013-07-09 00:00:00\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2010-07-06 00:00:00 to 2013-07-08 00:00:00\n",
      "Test period: 2013-07-08 00:00:00 to 2013-10-07 00:00:00\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2010-10-04 00:00:00 to 2013-10-04 00:00:00\n",
      "Test period: 2013-10-04 00:00:00 to 2014-01-07 00:00:00\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2011-01-03 00:00:00 to 2014-01-06 00:00:00\n",
      "Test period: 2014-01-06 00:00:00 to 2014-04-08 00:00:00\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "Train period: 2011-04-04 00:00:00 to 2014-04-07 00:00:00\n",
      "Test period: 2014-04-07 00:00:00 to 2014-07-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train_window = 252*3 # 3 years\n",
    "trade_window = 63 # 3 months\n",
    "algo_returns = {}\n",
    "top_copulas_dfs = []\n",
    "for i in range(train_window, len(prices)-trade_window, trade_window):\n",
    "    print(\"\\n##################################################################################\\n\")\n",
    "    print(\"Train period:\", prices.index[i-train_window], \"to\", prices.index[i])\n",
    "    print(\"Test period:\", prices.index[i], \"to\", prices.index[i+trade_window+1])\n",
    "    for sector in sectors_list:\n",
    "        ticker_cols = prices.columns.get_level_values(1).unique()\n",
    "        # find the indexing position of tickers_by_sectors[sector] in ticker_cols\n",
    "        indx = np.where(ticker_cols.isin(tickers_by_sectors[sector]))[0]\n",
    "        train_closes = prices.Close.iloc[i-train_window:i, indx]\n",
    "        train_opens = prices.Open.iloc[i-train_window:i, indx]\n",
    "        train = pd.concat([train_closes, train_opens], axis=1, keys=['Close', 'Open'])\n",
    "        returns_form = preprocess_data(train)\n",
    "        test_closes = prices.Close.iloc[i:i+trade_window+1, indx]\n",
    "        test_opens = prices.Open.iloc[i:i+trade_window+1, indx]\n",
    "        test = pd.concat([test_closes, test_opens], axis=1, keys=['Close', 'Open'])\n",
    "        returns_trade = preprocess_data(test)\n",
    "        # Get the fitted copula for the sector\n",
    "        results = formation(train, use_num=True, num_pairs=1)\n",
    "\n",
    "        selected_pair = list(results.keys())\n",
    "        cl = 0.99 # confidence level\n",
    "        fee = 0 # trading fee\n",
    "\n",
    "        for pair in selected_pairs:\n",
    "            s1,s2 = parse_pair(pair)\n",
    "                    \n",
    "            dist_s1 = results[pair]['s1']\n",
    "            dist_s2 = results[pair]['s2']\n",
    "            best_copula = results[pair]['copula']\n",
    "            print(f\"Pair: {pair}, Fitted copula: {best_copula.name}, in {sector.upper()}.\\n\")   \n",
    "            # calculate conditional probabilities\n",
    "            prob_s1 = []\n",
    "            prob_s2 = []\n",
    "\n",
    "            for u,v in zip(dist_s1.cdf(returns_trade[s1]), dist_s2.cdf(returns_trade[s2])):\n",
    "                prob_s1.append(best_copula.cdf_u_given_v(u,v))\n",
    "                prob_s2.append(best_copula.cdf_v_given_u(u,v))\n",
    "                \n",
    "            probs_trade = pd.DataFrame(np.vstack([prob_s1, prob_s2]).T, index=returns_trade.index, columns=[s1, s2])\n",
    "\n",
    "            # calculate positions\n",
    "            positions = pd.DataFrame(index=probs_trade.index, columns=probs_trade.columns)\n",
    "            long = False\n",
    "            short = False\n",
    "\n",
    "            for t in positions.index:    \n",
    "                # if long position is open\n",
    "                if long:\n",
    "                    if (probs_trade.loc[t][s1] >= 0.3) or (probs_trade.loc[t][s2] <= 0.7):\n",
    "                        positions.loc[t] = [0-fee, 0-fee] # Add fee to close positions\n",
    "                        long = False\n",
    "                    else:\n",
    "                        positions.loc[t] = [1, -1]\n",
    "\n",
    "                # if short position is open\n",
    "                elif short:\n",
    "                    if (probs_trade.loc[t][s1] <= 0.7) or (probs_trade.loc[t][s2] >= 0.3):\n",
    "                        positions.loc[t] = [0-fee, 0-fee]\n",
    "                        short = False\n",
    "                    else:\n",
    "                        positions.loc[t] = [-1, 1]\n",
    "\n",
    "                # if no positions are open\n",
    "                else:\n",
    "                    if (probs_trade.loc[t][s1] <= (1-cl)) and (probs_trade.loc[t][s2] >= cl):\n",
    "                        # open long position\n",
    "                        positions.loc[t] = [1-fee, -1-fee] # Add fee to open positions\n",
    "                        long = True\n",
    "                    elif (probs_trade.loc[t][s1] >= cl) and (probs_trade.loc[t][s2] <= (1-cl)):\n",
    "                        # open short positions\n",
    "                        positions.loc[t] = [-1-fee, 1-fee]\n",
    "                        short = True\n",
    "                    else:\n",
    "                        positions.loc[t] = [0, 0]\n",
    "                \n",
    "            # calculate returns\n",
    "            algo_ret = (returns_trade * positions.shift()).sum(axis=1)\n",
    "            # append algo_ret to algo_returns\n",
    "            if pair not in algo_returns.keys():\n",
    "                algo_returns[pair] = []\n",
    "            algo_returns[pair].append(algo_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copulas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
